2017-12-17 00:12:10.885038: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\platform\cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX AVX2
2017-12-17 00:12:11.324445: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1030] Found device 0 with properties: 
name: GeForce GTX 960M major: 5 minor: 0 memoryClockRate(GHz): 1.176
pciBusID: 0000:01:00.0
totalMemory: 2.00GiB freeMemory: 1.65GiB
2017-12-17 00:12:11.325023: I C:\tf_jenkins\home\workspace\rel-win\M\windows-gpu\PY\35\tensorflow\core\common_runtime\gpu\gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: GeForce GTX 960M, pci bus id: 0000:01:00.0, compute capability: 5.0)
5924
loss at step 1: 0.335182
loss at step 2: 0.328884
loss at step 3: 0.334064
loss at step 4: 0.330988
loss at step 5: 0.333190
loss at step 6: 0.332561
loss at step 7: 0.317011
loss at step 8: 0.319884
loss at step 9: 0.318885
loss at step 10: 0.322117
loss at step 11: 0.323807
loss at step 12: 0.340131
loss at step 13: 0.328144
loss at step 14: 0.334160
loss at step 15: 0.336762
loss at step 16: 0.326198
loss at step 17: 0.342490
loss at step 18: 0.330665
loss at step 19: 0.335345
loss at step 20: 0.335949
loss at step 21: 0.337091
loss at step 22: 0.332646
loss at step 23: 0.336079
loss at step 24: 0.331521
loss at step 25: 0.332015
loss at step 26: 0.336418
loss at step 27: 0.327193
loss at step 28: 0.342130
loss at step 29: 0.332084
loss at step 30: 0.333379
loss at step 31: 0.335251
loss at step 32: 0.333319
loss at step 33: 0.342262
loss at step 34: 0.348440
loss at step 35: 0.350091
loss at step 36: 0.347822
loss at step 37: 0.332993
loss at step 38: 0.335549
loss at step 39: 0.333129
loss at step 40: 0.337675
loss at step 41: 0.333501
loss at step 42: 0.343791
loss at step 43: 0.334909
loss at step 44: 0.332570
loss at step 45: 0.338284
loss at step 46: 0.329964
loss at step 47: 0.335442
loss at step 48: 0.332385
loss at step 49: 0.335752
loss at step 50: 0.339927
loss at step 51: 0.338481
loss at step 52: 0.334701
loss at step 53: 0.323740
loss at step 54: 0.320214
loss at step 55: 0.319115
loss at step 56: 0.321214
loss at step 57: 0.315289
loss at step 58: 0.342894
loss at step 59: 0.323594
loss at step 60: 0.323972
loss at step 61: 0.328604
loss at step 62: 0.321270
loss at step 63: 0.332436
loss at step 64: 0.325647
loss at step 65: 0.327298
loss at step 66: 0.324269
loss at step 67: 0.329910
loss at step 68: 0.320121
loss at step 69: 0.337910
loss at step 70: 0.325381
loss at step 71: 0.327356
loss at step 72: 0.326336
loss at step 73: 0.324705
loss at step 74: 0.334673
loss at step 75: 0.328180
loss at step 76: 0.328447
loss at step 77: 0.324924
loss at step 78: 0.330486
loss at step 79: 0.332174
loss at step 80: 0.345439
loss at step 81: 0.345276
loss at step 82: 0.343604
loss at step 83: 0.330359
loss at step 84: 0.327672
loss at step 85: 0.326834
loss at step 86: 0.332960
loss at step 87: 0.322520
loss at step 88: 0.342688
loss at step 89: 0.328283
loss at step 90: 0.328162
loss at step 91: 0.327747
loss at step 92: 0.331065
loss at step 93: 0.332592
loss at step 94: 0.336306
loss at step 95: 0.339698
loss at step 96: 0.343099
loss at step 97: 0.339547
loss at step 98: 0.341343
loss at step 99: 0.331946
loss at step 100: 0.323918
Average loss at step 100: 0.332144, Accuracy = 0.054999999701976776
loss at step 101: 0.320611
loss at step 102: 0.325353
loss at step 103: 0.319588
loss at step 104: 0.337320
loss at step 105: 0.324440
loss at step 106: 0.323287
loss at step 107: 0.325098
loss at step 108: 0.325397
loss at step 109: 0.326181
loss at step 110: 0.329810
loss at step 111: 0.323723
loss at step 112: 0.327093
loss at step 113: 0.329320
loss at step 114: 0.318759
loss at step 115: 0.335966
loss at step 116: 0.322545
loss at step 117: 0.326303
loss at step 118: 0.325234
loss at step 119: 0.324914
loss at step 120: 0.327711
loss at step 121: 0.332015
loss at step 122: 0.327540
loss at step 123: 0.325037
loss at step 124: 0.329873
loss at step 125: 0.320528
loss at step 126: 0.348895
loss at step 127: 0.341971
loss at step 128: 0.343296
loss at step 129: 0.333219
loss at step 130: 0.326254
loss at step 131: 0.324831
loss at step 132: 0.328377
loss at step 133: 0.326265
loss at step 134: 0.336332
loss at step 135: 0.327431
loss at step 136: 0.327803
loss at step 137: 0.323878
loss at step 138: 0.331077
loss at step 139: 0.327376
loss at step 140: 0.339103
loss at step 141: 0.340038
loss at step 142: 0.341131
loss at step 143: 0.340776
loss at step 144: 0.342703
loss at step 145: 0.337187
loss at step 146: 0.320476
loss at step 147: 0.323288
loss at step 148: 0.321130
loss at step 149: 0.325604
loss at step 150: 0.329442
loss at step 151: 0.330505
loss at step 152: 0.322664
loss at step 153: 0.324645
loss at step 154: 0.327664
loss at step 155: 0.319613
loss at step 156: 0.334883
loss at step 157: 0.323932
loss at step 158: 0.325639
loss at step 159: 0.325522
loss at step 160: 0.324833
loss at step 161: 0.328492
loss at step 162: 0.327788
loss at step 163: 0.323594
loss at step 164: 0.325568
loss at step 165: 0.325412
loss at step 166: 0.320513
loss at step 167: 0.336159
loss at step 168: 0.326624
loss at step 169: 0.323554
loss at step 170: 0.329798
loss at step 171: 0.323602
loss at step 172: 0.339584
loss at step 173: 0.341704
loss at step 174: 0.342553
loss at step 175: 0.337946
loss at step 176: 0.324631
loss at step 177: 0.325316
loss at step 178: 0.323536
loss at step 179: 0.330055
loss at step 180: 0.329342
loss at step 181: 0.331703
loss at step 182: 0.325980
loss at step 183: 0.324754
loss at step 184: 0.330954
loss at step 185: 0.320415
loss at step 186: 0.340374
loss at step 187: 0.337286
loss at step 188: 0.343629
loss at step 189: 0.341503
loss at step 190: 0.342677
loss at step 191: 0.339057
loss at step 192: 0.324981
loss at step 193: 0.321878
loss at step 194: 0.321063
loss at step 195: 0.324971
loss at step 196: 0.322097
loss at step 197: 0.336562
loss at step 198: 0.322133
loss at step 199: 0.324509
loss at step 200: 0.328848
Average loss at step 200: 0.329126, Accuracy = 0.06750000268220901
loss at step 201: 0.318904
loss at step 202: 0.333514
loss at step 203: 0.321285
loss at step 204: 0.324044
loss at step 205: 0.323358
loss at step 206: 0.327148
loss at step 207: 0.321525
loss at step 208: 0.332313
loss at step 209: 0.324857
loss at step 210: 0.323150
loss at step 211: 0.326078
loss at step 212: 0.320130
loss at step 213: 0.336584
loss at step 214: 0.324691
loss at step 215: 0.324443
loss at step 216: 0.324492
loss at step 217: 0.328988
loss at step 218: 0.333681
loss at step 219: 0.340856
loss at step 220: 0.341720
loss at step 221: 0.340394
loss at step 222: 0.324046
loss at step 223: 0.324119
loss at step 224: 0.323764
loss at step 225: 0.328565
loss at step 226: 0.322744
loss at step 227: 0.336234
loss at step 228: 0.324642
loss at step 229: 0.324476
loss at step 230: 0.327535
loss at step 231: 0.324415
loss at step 232: 0.334262
loss at step 233: 0.337037
loss at step 234: 0.340829
loss at step 235: 0.343871
loss at step 236: 0.341832
loss at step 237: 0.340433
loss at step 238: 0.328767
loss at step 239: 0.322649
loss at step 240: 0.320650
loss at step 241: 0.325912
loss at step 242: 0.318712
loss at step 243: 0.339222
loss at step 244: 0.320911
loss at step 245: 0.322512
loss at step 246: 0.327226
loss at step 247: 0.324233
loss at step 248: 0.326973
loss at step 249: 0.324741
loss at step 250: 0.322546
loss at step 251: 0.323551
loss at step 252: 0.327287
loss at step 253: 0.317647
loss at step 254: 0.334580
loss at step 255: 0.322772
loss at step 256: 0.324899
loss at step 257: 0.322329
loss at step 258: 0.324210
loss at step 259: 0.331003
loss at step 260: 0.328171
loss at step 261: 0.324316
loss at step 262: 0.323320
loss at step 263: 0.328501
loss at step 264: 0.325990
loss at step 265: 0.344005
loss at step 266: 0.340028
loss at step 267: 0.340668
loss at step 268: 0.328911
loss at step 269: 0.322151
loss at step 270: 0.323118
loss at step 271: 0.328805
loss at step 272: 0.320731
loss at step 273: 0.337053
loss at step 274: 0.322428
loss at step 275: 0.324978
loss at step 276: 0.321561
loss at step 277: 0.328906
loss at step 278: 0.329738
loss at step 279: 0.337751
loss at step 280: 0.339874
loss at step 281: 0.342077
loss at step 282: 0.340762
loss at step 283: 0.341444
loss at step 284: 0.334412
loss at step 285: 0.321407
loss at step 286: 0.320999
loss at step 287: 0.323596
loss at step 288: 0.322194
loss at step 289: 0.332409
loss at step 290: 0.326186
loss at step 291: 0.322260
loss at step 292: 0.323442
loss at step 293: 0.326393
loss at step 294: 0.321595
loss at step 295: 0.329877
loss at step 296: 0.320882
loss at step 297: 0.324762
loss at step 298: 0.326449
loss at step 299: 0.319837
loss at step 300: 0.331587
Average loss at step 300: 0.328179, Accuracy = 0.1574999988079071
loss at step 301: 0.322978
loss at step 302: 0.323608
loss at step 303: 0.323559
loss at step 304: 0.324051
loss at step 305: 0.323793
loss at step 306: 0.332185
loss at step 307: 0.325770
loss at step 308: 0.322630
loss at step 309: 0.330382
loss at step 310: 0.320284
loss at step 311: 0.342187
loss at step 312: 0.339699
loss at step 313: 0.340513
loss at step 314: 0.333038
loss at step 315: 0.323207
loss at step 316: 0.322071
loss at step 317: 0.323347
loss at step 318: 0.327488
loss at step 319: 0.329963
loss at step 320: 0.326120
loss at step 321: 0.324131
loss at step 322: 0.321785
loss at step 323: 0.328799
loss at step 324: 0.320163
loss at step 325: 0.342515
loss at step 326: 0.339105
loss at step 327: 0.343194
loss at step 328: 0.339713
loss at step 329: 0.343065
loss at step 330: 0.337598
loss at step 331: 0.321383
loss at step 332: 0.322100
loss at step 333: 0.319678
loss at step 334: 0.324934
loss at step 335: 0.326498
loss at step 336: 0.332368
loss at step 337: 0.320095
loss at step 338: 0.324043
loss at step 339: 0.327881
loss at step 340: 0.318517
loss at step 341: 0.331964
loss at step 342: 0.320409
loss at step 343: 0.322681
loss at step 344: 0.322865
loss at step 345: 0.325129
loss at step 346: 0.324768
loss at step 347: 0.327119
loss at step 348: 0.323919
loss at step 349: 0.322627
loss at step 350: 0.326101
loss at step 351: 0.318807
loss at step 352: 0.334664
loss at step 353: 0.324603
loss at step 354: 0.322680
loss at step 355: 0.326156
loss at step 356: 0.324791
loss at step 357: 0.335582
loss at step 358: 0.339561
loss at step 359: 0.340551
loss at step 360: 0.337262
loss at step 361: 0.321271
loss at step 362: 0.323346
loss at step 363: 0.320878
loss at step 364: 0.327373
loss at step 365: 0.324010
loss at step 366: 0.331619
loss at step 367: 0.323718
loss at step 368: 0.321716
loss at step 369: 0.328551
loss at step 370: 0.319386
loss at step 371: 0.338061
loss at step 372: 0.337393
loss at step 373: 0.340883
loss at step 374: 0.343167
loss at step 375: 0.342512
loss at step 376: 0.339014
loss at step 377: 0.325786
loss at step 378: 0.322114
loss at step 379: 0.321103
loss at step 380: 0.324254
loss at step 381: 0.319658
loss at step 382: 0.339622
loss at step 383: 0.320868
loss at step 384: 0.321893
loss at step 385: 0.328206
loss at step 386: 0.320725
loss at step 387: 0.330336
loss at step 388: 0.319480
loss at step 389: 0.322734
loss at step 390: 0.320680
loss at step 391: 0.326894
loss at step 392: 0.318153
loss at step 393: 0.332696
loss at step 394: 0.321914
loss at step 395: 0.323811
loss at step 396: 0.324087
loss at step 397: 0.322152
loss at step 398: 0.332847
loss at step 399: 0.323566
loss at step 400: 0.324064
Average loss at step 400: 0.327572, Accuracy = 0.05000000074505806
loss at step 401: 0.321561
loss at step 402: 0.326743
loss at step 403: 0.330419
loss at step 404: 0.340616
loss at step 405: 0.341423
loss at step 406: 0.338999
loss at step 407: 0.323770
loss at step 408: 0.321727
loss at step 409: 0.321997
loss at step 410: 0.327930
loss at step 411: 0.318455
loss at step 412: 0.337187
loss at step 413: 0.322525
loss at step 414: 0.322821
loss at step 415: 0.323832
loss at step 416: 0.326303
loss at step 417: 0.330774
loss at step 418: 0.336425
loss at step 419: 0.340204
loss at step 420: 0.343110
loss at step 421: 0.339798
loss at step 422: 0.341838
loss at step 423: 0.330220
loss at step 424: 0.323557
loss at step 425: 0.319981
loss at step 426: 0.325528
loss at step 427: 0.318618
loss at step 428: 0.337574
loss at step 429: 0.321706
loss at step 430: 0.321713
loss at step 431: 0.323552
loss at step 432: 0.324150
loss at step 433: 0.325238
loss at step 434: 0.325008
loss at step 435: 0.320065
loss at step 436: 0.323314
loss at step 437: 0.326710
loss at step 438: 0.315992
loss at step 439: 0.334353
loss at step 440: 0.320129
loss at step 441: 0.324266
loss at step 442: 0.322864
loss at step 443: 0.323002
loss at step 444: 0.326613
loss at step 445: 0.327763
loss at step 446: 0.323877
loss at step 447: 0.321747
loss at step 448: 0.326877
loss at step 449: 0.319556
loss at step 450: 0.344777
loss at step 451: 0.338823
loss at step 452: 0.340224
loss at step 453: 0.328157
loss at step 454: 0.321737
loss at step 455: 0.320569
loss at step 456: 0.324937
loss at step 457: 0.323031
loss at step 458: 0.333267
loss at step 459: 0.322249
loss at step 460: 0.324147
loss at step 461: 0.319371
loss at step 462: 0.327351
loss at step 463: 0.325652
loss at step 464: 0.338639
loss at step 465: 0.339559
loss at step 466: 0.341168
loss at step 467: 0.339820
loss at step 468: 0.342149
loss at step 469: 0.335701
loss at step 470: 0.321132
loss at step 471: 0.321596
loss at step 472: 0.320494
loss at step 473: 0.323830
loss at step 474: 0.329478
loss at step 475: 0.327899
loss at step 476: 0.319683
loss at step 477: 0.322326
loss at step 478: 0.324979
loss at step 479: 0.319616
loss at step 480: 0.330296
loss at step 481: 0.319746
loss at step 482: 0.322827
loss at step 483: 0.322965
loss at step 484: 0.322365
loss at step 485: 0.327745
loss at step 486: 0.324602
loss at step 487: 0.321821
loss at step 488: 0.323743
loss at step 489: 0.323652
loss at step 490: 0.320412
loss at step 491: 0.331344
loss at step 492: 0.324126
loss at step 493: 0.320918
loss at step 494: 0.327485
loss at step 495: 0.319974
loss at step 496: 0.339019
loss at step 497: 0.338705
loss at step 498: 0.340193
loss at step 499: 0.333639
loss at step 500: 0.320783
Average loss at step 500: 0.327511, Accuracy = 0.32249999046325684
loss at step 501: 0.321224
loss at step 502: 0.320097
loss at step 503: 0.326274
loss at step 504: 0.327733
loss at step 505: 0.327032
loss at step 506: 0.322463
loss at step 507: 0.321514
loss at step 508: 0.328042
loss at step 509: 0.317868
loss at step 510: 0.340391
loss at step 511: 0.336549
loss at step 512: 0.343204
loss at step 513: 0.340267
loss at step 514: 0.341074
loss at step 515: 0.338273
loss at step 516: 0.324249
loss at step 517: 0.321645
loss at step 518: 0.319676
loss at step 519: 0.323981
loss at step 520: 0.322417
loss at step 521: 0.335071
loss at step 522: 0.319974
loss at step 523: 0.321655
loss at step 524: 0.326521
loss at step 525: 0.317686
loss at step 526: 0.331568
loss at step 527: 0.318937
loss at step 528: 0.321377
loss at step 529: 0.321203
loss at step 530: 0.326528
loss at step 531: 0.320513
loss at step 532: 0.329990
loss at step 533: 0.322587
loss at step 534: 0.321477
loss at step 535: 0.325080
loss at step 536: 0.318746
loss at step 537: 0.333007
loss at step 538: 0.321904
loss at step 539: 0.322018
loss at step 540: 0.322506
loss at step 541: 0.325839
loss at step 542: 0.331980
loss at step 543: 0.339041
loss at step 544: 0.339699
loss at step 545: 0.337975
loss at step 546: 0.320613
loss at step 547: 0.321396
loss at step 548: 0.319977
loss at step 549: 0.326058
loss at step 550: 0.321355
loss at step 551: 0.332899
loss at step 552: 0.322255
loss at step 553: 0.320811
loss at step 554: 0.325246
loss at step 555: 0.321701
loss at step 556: 0.333860
loss at step 557: 0.336179
loss at step 558: 0.339623
loss at step 559: 0.343120
loss at step 560: 0.341379
loss at step 561: 0.338475
loss at step 562: 0.327853
loss at step 563: 0.322636
loss at step 564: 0.320453
loss at step 565: 0.323990
loss at step 566: 0.318290
loss at step 567: 0.338487
loss at step 568: 0.318709
loss at step 569: 0.320744
loss at step 570: 0.324712
loss at step 571: 0.321842
loss at step 572: 0.326175
loss at step 573: 0.322111
loss at step 574: 0.320634
loss at step 575: 0.321041
loss at step 576: 0.326192
loss at step 577: 0.316338
loss at step 578: 0.333963
loss at step 579: 0.320034
loss at step 580: 0.323223
loss at step 581: 0.320994
loss at step 582: 0.323216
loss at step 583: 0.328301
loss at step 584: 0.324422
loss at step 585: 0.322385
loss at step 586: 0.320017
loss at step 587: 0.326075
loss at step 588: 0.325442
loss at step 589: 0.341604
loss at step 590: 0.338949
loss at step 591: 0.339023
loss at step 592: 0.325643
loss at step 593: 0.318976
loss at step 594: 0.320432
loss at step 595: 0.327095
loss at step 596: 0.318733
loss at step 597: 0.335273
loss at step 598: 0.319881
loss at step 599: 0.322549
loss at step 600: 0.319172
Average loss at step 600: 0.326474, Accuracy = 0.27250000834465027
loss at step 601: 0.326845
loss at step 602: 0.329240
loss at step 603: 0.336765
loss at step 604: 0.338861
loss at step 605: 0.341768
loss at step 606: 0.338862
loss at step 607: 0.340223
loss at step 608: 0.333134
loss at step 609: 0.321945
loss at step 610: 0.320733
loss at step 611: 0.322694
loss at step 612: 0.321238
loss at step 613: 0.331999
loss at step 614: 0.323964
loss at step 615: 0.320126
loss at step 616: 0.320807
loss at step 617: 0.324375
loss at step 618: 0.321175
loss at step 619: 0.328381
loss at step 620: 0.318859
loss at step 621: 0.322545
loss at step 622: 0.324879
loss at step 623: 0.318177
loss at step 624: 0.331871
loss at step 625: 0.319896
loss at step 626: 0.322200
loss at step 627: 0.321903
loss at step 628: 0.322366
loss at step 629: 0.322964
loss at step 630: 0.328236
loss at step 631: 0.323809
loss at step 632: 0.320952
loss at step 633: 0.327161
loss at step 634: 0.317634
loss at step 635: 0.341766
loss at step 636: 0.338194
loss at step 637: 0.339516
loss at step 638: 0.329993
loss at step 639: 0.320685
loss at step 640: 0.320065
loss at step 641: 0.322200
loss at step 642: 0.324492
loss at step 643: 0.329187
loss at step 644: 0.323782
loss at step 645: 0.321811
loss at step 646: 0.319371
loss at step 647: 0.327029
loss at step 648: 0.319570
loss at step 649: 0.341001
loss at step 650: 0.338833
loss at step 651: 0.341810
loss at step 652: 0.338574
loss at step 653: 0.341807
loss at step 654: 0.336767
loss at step 655: 0.320714
loss at step 656: 0.322478
loss at step 657: 0.319125
loss at step 658: 0.324107
loss at step 659: 0.326041
loss at step 660: 0.330263
loss at step 661: 0.318290
loss at step 662: 0.321653
loss at step 663: 0.325420
loss at step 664: 0.317458
loss at step 665: 0.330391
loss at step 666: 0.319118
loss at step 667: 0.320733
loss at step 668: 0.322183
loss at step 669: 0.324165
loss at step 670: 0.324615
loss at step 671: 0.325706
loss at step 672: 0.321165
loss at step 673: 0.321845
loss at step 674: 0.323939
loss at step 675: 0.317927
loss at step 676: 0.331954
loss at step 677: 0.322900
loss at step 678: 0.319759
loss at step 679: 0.325018
loss at step 680: 0.321730
loss at step 681: 0.335375
loss at step 682: 0.338744
loss at step 683: 0.340042
loss at step 684: 0.335416
loss at step 685: 0.319942
loss at step 686: 0.321962
loss at step 687: 0.319152
loss at step 688: 0.326277
loss at step 689: 0.323625
loss at step 690: 0.329644
loss at step 691: 0.321328
loss at step 692: 0.320110
loss at step 693: 0.326429
loss at step 694: 0.317381
loss at step 695: 0.337592
loss at step 696: 0.335992
loss at step 697: 0.341052
loss at step 698: 0.341325
loss at step 699: 0.340907
loss at step 700: 0.337769
Average loss at step 700: 0.327118, Accuracy = 0.550000011920929
loss at step 701: 0.324607
loss at step 702: 0.322254
loss at step 703: 0.320370
loss at step 704: 0.323011
loss at step 705: 0.319361
loss at step 706: 0.336882
loss at step 707: 0.318993
loss at step 708: 0.319734
loss at step 709: 0.326250
loss at step 710: 0.317482
loss at step 711: 0.329931
loss at step 712: 0.318603
loss at step 713: 0.320545
loss at step 714: 0.319499
loss at step 715: 0.325020
loss at step 716: 0.318505
loss at step 717: 0.332012
loss at step 718: 0.321547
loss at step 719: 0.322511
loss at step 720: 0.323498
loss at step 721: 0.320642
loss at step 722: 0.332735
loss at step 723: 0.321687
loss at step 724: 0.322913
loss at step 725: 0.320644
loss at step 726: 0.325713
loss at step 727: 0.331184
loss at step 728: 0.339056
loss at step 729: 0.340510
loss at step 730: 0.337836
loss at step 731: 0.321701
loss at step 732: 0.320191
loss at step 733: 0.320105
loss at step 734: 0.326142
loss at step 735: 0.317947
loss at step 736: 0.333595
loss at step 737: 0.319816
loss at step 738: 0.320342
loss at step 739: 0.321785
loss at step 740: 0.323646
loss at step 741: 0.330637
loss at step 742: 0.335829
loss at step 743: 0.339599
loss at step 744: 0.341551
loss at step 745: 0.339236
loss at step 746: 0.340036
loss at step 747: 0.329145
loss at step 748: 0.322877
loss at step 749: 0.319006
loss at step 750: 0.325719
loss at step 751: 0.317471
loss at step 752: 0.336302
loss at step 753: 0.318987
loss at step 754: 0.319946
loss at step 755: 0.322749
loss at step 756: 0.323226
loss at step 757: 0.323664
loss at step 758: 0.323931
loss at step 759: 0.319093
loss at step 760: 0.322424
loss at step 761: 0.326574
loss at step 762: 0.315233
loss at step 763: 0.333247
loss at step 764: 0.318484
loss at step 765: 0.323298
loss at step 766: 0.320882
loss at step 767: 0.321619
loss at step 768: 0.326250
loss at step 769: 0.325974
loss at step 770: 0.321999
loss at step 771: 0.320126
loss at step 772: 0.325824
loss at step 773: 0.319812
loss at step 774: 0.342834
loss at step 775: 0.337448
loss at step 776: 0.338959
loss at step 777: 0.326023
loss at step 778: 0.320437
loss at step 779: 0.319341
loss at step 780: 0.323725
loss at step 781: 0.321163
loss at step 782: 0.332373
loss at step 783: 0.319839
loss at step 784: 0.321920
loss at step 785: 0.318105
loss at step 786: 0.325712
loss at step 787: 0.324736
loss at step 788: 0.338154
loss at step 789: 0.339113
loss at step 790: 0.340278
loss at step 791: 0.339606
loss at step 792: 0.340586
loss at step 793: 0.334763
loss at step 794: 0.320999
loss at step 795: 0.320646
loss at step 796: 0.320990
loss at step 797: 0.323127
loss at step 798: 0.329605
loss at step 799: 0.325860
loss at step 800: 0.318782
Average loss at step 800: 0.325947, Accuracy = 0.07249999791383743
loss at step 801: 0.320912
loss at step 802: 0.323578
loss at step 803: 0.319217
loss at step 804: 0.328811
loss at step 805: 0.319071
loss at step 806: 0.321985
loss at step 807: 0.322938
loss at step 808: 0.320939
loss at step 809: 0.328215
loss at step 810: 0.322491
loss at step 811: 0.320183
loss at step 812: 0.322046
loss at step 813: 0.322514
loss at step 814: 0.320061
loss at step 815: 0.329164
loss at step 816: 0.323213
loss at step 817: 0.319634
loss at step 818: 0.327111
loss at step 819: 0.318360
loss at step 820: 0.339392
loss at step 821: 0.337338
loss at step 822: 0.338341
loss at step 823: 0.331828
loss at step 824: 0.319468
loss at step 825: 0.320050
loss at step 826: 0.319118
loss at step 827: 0.325489
loss at step 828: 0.326955
loss at step 829: 0.324300
loss at step 830: 0.320671
loss at step 831: 0.319641
loss at step 832: 0.326423
loss at step 833: 0.316471
loss at step 834: 0.341109
loss at step 835: 0.336021
loss at step 836: 0.343311
loss at step 837: 0.338972
loss at step 838: 0.340224
loss at step 839: 0.337755
loss at step 840: 0.322846
loss at step 841: 0.321803
loss at step 842: 0.319233
loss at step 843: 0.323067
loss at step 844: 0.322416
loss at step 845: 0.332804
loss at step 846: 0.318784
loss at step 847: 0.320905
loss at step 848: 0.325100
loss at step 849: 0.316107
loss at step 850: 0.330872
loss at step 851: 0.318112
loss at step 852: 0.320693
loss at step 853: 0.320144
loss at step 854: 0.325630
loss at step 855: 0.320761
loss at step 856: 0.328337
loss at step 857: 0.321222
loss at step 858: 0.320144
loss at step 859: 0.324217
loss at step 860: 0.317327
loss at step 861: 0.332051
loss at step 862: 0.321009
loss at step 863: 0.320724
loss at step 864: 0.322109
loss at step 865: 0.324630
loss at step 866: 0.331875
loss at step 867: 0.338220
loss at step 868: 0.338721
loss at step 869: 0.336920
loss at step 870: 0.319012
loss at step 871: 0.321182
loss at step 872: 0.318182
loss at step 873: 0.325151
loss at step 874: 0.321115
loss at step 875: 0.331066
loss at step 876: 0.321468
loss at step 877: 0.319056
loss at step 878: 0.324343
loss at step 879: 0.319439
loss at step 880: 0.334271
loss at step 881: 0.336087
loss at step 882: 0.339744
loss at step 883: 0.341539
loss at step 884: 0.341047
loss at step 885: 0.337726
loss at step 886: 0.327134
loss at step 887: 0.322054
loss at step 888: 0.320657
loss at step 889: 0.323261
loss at step 890: 0.317759
loss at step 891: 0.337402
loss at step 892: 0.317709
loss at step 893: 0.320567
loss at step 894: 0.323441
loss at step 895: 0.319936
loss at step 896: 0.326527
loss at step 897: 0.320371
loss at step 898: 0.320447
loss at step 899: 0.319720
loss at step 900: 0.325502
Average loss at step 900: 0.325630, Accuracy = 0.05249999836087227
loss at step 901: 0.316120
loss at step 902: 0.332602
loss at step 903: 0.318987
loss at step 904: 0.321789
loss at step 905: 0.320487
loss at step 906: 0.321953
loss at step 907: 0.328367
loss at step 908: 0.322338
loss at step 909: 0.321619
loss at step 910: 0.318978
loss at step 911: 0.325707
loss at step 912: 0.325896
loss at step 913: 0.339897
loss at step 914: 0.338469
loss at step 915: 0.338183
loss at step 916: 0.322872
loss at step 917: 0.318072
loss at step 918: 0.318740
loss at step 919: 0.326870
loss at step 920: 0.317047
loss at step 921: 0.333853
loss at step 922: 0.318299
loss at step 923: 0.321067
loss at step 924: 0.318690
loss at step 925: 0.324754
loss at step 926: 0.329258
loss at step 927: 0.336077
loss at step 928: 0.338731
loss at step 929: 0.341997
loss at step 930: 0.337898
loss at step 931: 0.339938
loss at step 932: 0.331828
loss at step 933: 0.322378
loss at step 934: 0.320058
loss at step 935: 0.323163
loss at step 936: 0.320080
loss at step 937: 0.332189
loss at step 938: 0.321966
loss at step 939: 0.319267
loss at step 940: 0.319594
loss at step 941: 0.323106
loss at step 942: 0.321394
loss at step 943: 0.326401
loss at step 944: 0.318652
loss at step 945: 0.322289
loss at step 946: 0.324061
loss at step 947: 0.317274
loss at step 948: 0.331427
loss at step 949: 0.318345
loss at step 950: 0.321252
loss at step 951: 0.320414
loss at step 952: 0.321282
loss at step 953: 0.323057
loss at step 954: 0.326902
loss at step 955: 0.322678
loss at step 956: 0.320795
loss at step 957: 0.325866
loss at step 958: 0.317249
loss at step 959: 0.341798
loss at step 960: 0.336964
loss at step 961: 0.339242
loss at step 962: 0.327807
loss at step 963: 0.319219
loss at step 964: 0.318892
loss at step 965: 0.321680
loss at step 966: 0.322926
loss at step 967: 0.328821
loss at step 968: 0.321701
loss at step 969: 0.320694
loss at step 970: 0.317905
loss at step 971: 0.325884
loss at step 972: 0.320174
loss at step 973: 0.339768
loss at step 974: 0.338829
loss at step 975: 0.340675
loss at step 976: 0.338291
loss at step 977: 0.340984
loss at step 978: 0.336440
loss at step 979: 0.319843
loss at step 980: 0.322192
loss at step 981: 0.319135
loss at step 982: 0.324128
loss at step 983: 0.325425
loss at step 984: 0.328351
loss at step 985: 0.317932
loss at step 986: 0.320365
loss at step 987: 0.324069
loss at step 988: 0.316527
loss at step 989: 0.329755
loss at step 990: 0.319251
loss at step 991: 0.320023
loss at step 992: 0.321995
loss at step 993: 0.323238
loss at step 994: 0.324586
loss at step 995: 0.324362
loss at step 996: 0.319108
loss at step 997: 0.321524
loss at step 998: 0.322041
loss at step 999: 0.316961
loss at step 1000: 0.331785
Average loss at step 1000: 0.325398, Accuracy = 0.06750000268220901
loss at step 1001: 0.322275
loss at step 1002: 0.319276
loss at step 1003: 0.325236
loss at step 1004: 0.321066
loss at step 1005: 0.334877
loss at step 1006: 0.337644
loss at step 1007: 0.339056
loss at step 1008: 0.333458
loss at step 1009: 0.318890
loss at step 1010: 0.320043
loss at step 1011: 0.318110
loss at step 1012: 0.325403
loss at step 1013: 0.323417
loss at step 1014: 0.327019
loss at step 1015: 0.320160
loss at step 1016: 0.319331
loss at step 1017: 0.325749
loss at step 1018: 0.315415
loss at step 1019: 0.338353
loss at step 1020: 0.335715
loss at step 1021: 0.341336
loss at step 1022: 0.340301
loss at step 1023: 0.340201
loss at step 1024: 0.338067
loss at step 1025: 0.324396
loss at step 1026: 0.322150
loss at step 1027: 0.320063
loss at step 1028: 0.322845
loss at step 1029: 0.319952
loss at step 1030: 0.335102
loss at step 1031: 0.318128
loss at step 1032: 0.319722
loss at step 1033: 0.325167
loss at step 1034: 0.316097
loss at step 1035: 0.329918
loss at step 1036: 0.317728
loss at step 1037: 0.320265
loss at step 1038: 0.319760
loss at step 1039: 0.324360
loss at step 1040: 0.319069
loss at step 1041: 0.329161
loss at step 1042: 0.320150
loss at step 1043: 0.320193
loss at step 1044: 0.322002
loss at step 1045: 0.319090
loss at step 1046: 0.331620
loss at step 1047: 0.319091
loss at step 1048: 0.321467
loss at step 1049: 0.319235
loss at step 1050: 0.325233
loss at step 1051: 0.330802
loss at step 1052: 0.337099
loss at step 1053: 0.339260
loss at step 1054: 0.336624
loss at step 1055: 0.320103
loss at step 1056: 0.319016
loss at step 1057: 0.318956
loss at step 1058: 0.325098
loss at step 1059: 0.318041
loss at step 1060: 0.333138
loss at step 1061: 0.318882
loss at step 1062: 0.319709
loss at step 1063: 0.321978
loss at step 1064: 0.322346
loss at step 1065: 0.330773
loss at step 1066: 0.335427
loss at step 1067: 0.339312
loss at step 1068: 0.341045
loss at step 1069: 0.339368
loss at step 1070: 0.339096
loss at step 1071: 0.327991
loss at step 1072: 0.322859
loss at step 1073: 0.319097
loss at step 1074: 0.325477
loss at step 1075: 0.317198
loss at step 1076: 0.336046
loss at step 1077: 0.317389
loss at step 1078: 0.318818
loss at step 1079: 0.322432
loss at step 1080: 0.321974
loss at step 1081: 0.323085
loss at step 1082: 0.322300
loss at step 1083: 0.318691
loss at step 1084: 0.321659
loss at step 1085: 0.325700
loss at step 1086: 0.314519
loss at step 1087: 0.332407
loss at step 1088: 0.317681
loss at step 1089: 0.322551
loss at step 1090: 0.319138
loss at step 1091: 0.320635
loss at step 1092: 0.326584
loss at step 1093: 0.324946
loss at step 1094: 0.320974
loss at step 1095: 0.319358
loss at step 1096: 0.325459
loss at step 1097: 0.320853
loss at step 1098: 0.341569
loss at step 1099: 0.336598
loss at step 1100: 0.338311
Average loss at step 1100: 0.325707, Accuracy = 0.042500000447034836
loss at step 1101: 0.324952
loss at step 1102: 0.318813
loss at step 1103: 0.318430
loss at step 1104: 0.323789
loss at step 1105: 0.319820
loss at step 1106: 0.332189
loss at step 1107: 0.318713
loss at step 1108: 0.320591
loss at step 1109: 0.317489
loss at step 1110: 0.325348
loss at step 1111: 0.325045
loss at step 1112: 0.337435
loss at step 1113: 0.338095
loss at step 1114: 0.340304
loss at step 1115: 0.338397
loss at step 1116: 0.339517
loss at step 1117: 0.334233
loss at step 1118: 0.321222
loss at step 1119: 0.320001
loss at step 1120: 0.320857
loss at step 1121: 0.321657
loss at step 1122: 0.329439
loss at step 1123: 0.325669
loss at step 1124: 0.319493
loss at step 1125: 0.319759
loss at step 1126: 0.322718
loss at step 1127: 0.317435
loss at step 1128: 0.330067
loss at step 1129: 0.321152
loss at step 1130: 0.323169
loss at step 1131: 0.323372
loss at step 1132: 0.319947
loss at step 1133: 0.328750
loss at step 1134: 0.323924
loss at step 1135: 0.321736
loss at step 1136: 0.322290
loss at step 1137: 0.321813
loss at step 1138: 0.320121
loss at step 1139: 0.330442
loss at step 1140: 0.323883
loss at step 1141: 0.319878
loss at step 1142: 0.328003
loss at step 1143: 0.317719
loss at step 1144: 0.339575
loss at step 1145: 0.337062
loss at step 1146: 0.337642
loss at step 1147: 0.329714
loss at step 1148: 0.318164
loss at step 1149: 0.318736
loss at step 1150: 0.317846
loss at step 1151: 0.324951
loss at step 1152: 0.325843
loss at step 1153: 0.322371
loss at step 1154: 0.319297
loss at step 1155: 0.318611
loss at step 1156: 0.324876
loss at step 1157: 0.315661
loss at step 1158: 0.341310
loss at step 1159: 0.336845
loss at step 1160: 0.342793
loss at step 1161: 0.337987
loss at step 1162: 0.340058
loss at step 1163: 0.337467
loss at step 1164: 0.322779
loss at step 1165: 0.321908
loss at step 1166: 0.319254
loss at step 1167: 0.323528
loss at step 1168: 0.322558
loss at step 1169: 0.333544
loss at step 1170: 0.317795
loss at step 1171: 0.321137
loss at step 1172: 0.324742
loss at step 1173: 0.315463
loss at step 1174: 0.331039
loss at step 1175: 0.318781
loss at step 1176: 0.319976
loss at step 1177: 0.319973
loss at step 1178: 0.324734
loss at step 1179: 0.320712
loss at step 1180: 0.328096
loss at step 1181: 0.322201
loss at step 1182: 0.319854
loss at step 1183: 0.324249
loss at step 1184: 0.316399
loss at step 1185: 0.332271
loss at step 1186: 0.322373
loss at step 1187: 0.321630
loss at step 1188: 0.322833
loss at step 1189: 0.323374
loss at step 1190: 0.332874
loss at step 1191: 0.339109
loss at step 1192: 0.339093
loss at step 1193: 0.338064
loss at step 1194: 0.321032
loss at step 1195: 0.322071
loss at step 1196: 0.318480
loss at step 1197: 0.324124
loss at step 1198: 0.320798
loss at step 1199: 0.329313
loss at step 1200: 0.319836
Average loss at step 1200: 0.325505, Accuracy = 0.11749999970197678
loss at step 1201: 0.316880
loss at step 1202: 0.322284
loss at step 1203: 0.316367
loss at step 1204: 0.334681
loss at step 1205: 0.337019
loss at step 1206: 0.340108
loss at step 1207: 0.342047
loss at step 1208: 0.341334
loss at step 1209: 0.337863
loss at step 1210: 0.327185
loss at step 1211: 0.322935
loss at step 1212: 0.321732
loss at step 1213: 0.324662
loss at step 1214: 0.319162
loss at step 1215: 0.337654
loss at step 1216: 0.317844
loss at step 1217: 0.320890
loss at step 1218: 0.325195
loss at step 1219: 0.319976
loss at step 1220: 0.327696
loss at step 1221: 0.320066
loss at step 1222: 0.321342
loss at step 1223: 0.319927
loss at step 1224: 0.326471
loss at step 1225: 0.316404
loss at step 1226: 0.332077
loss at step 1227: 0.318958
loss at step 1228: 0.321713
loss at step 1229: 0.320753
loss at step 1230: 0.321108
loss at step 1231: 0.328997
loss at step 1232: 0.321377
loss at step 1233: 0.320958
loss at step 1234: 0.318358
loss at step 1235: 0.324896
loss at step 1236: 0.326333
loss at step 1237: 0.338724
loss at step 1238: 0.338019
loss at step 1239: 0.337187
loss at step 1240: 0.321123
loss at step 1241: 0.316954
loss at step 1242: 0.317031
loss at step 1243: 0.325665
loss at step 1244: 0.315527
loss at step 1245: 0.333999
loss at step 1246: 0.317550
loss at step 1247: 0.319145
loss at step 1248: 0.318808
loss at step 1249: 0.323339
loss at step 1250: 0.329112
loss at step 1251: 0.335218
loss at step 1252: 0.338681
loss at step 1253: 0.342029
loss at step 1254: 0.337622
loss at step 1255: 0.339476
loss at step 1256: 0.331331
loss at step 1257: 0.323239
loss at step 1258: 0.319273
loss at step 1259: 0.323670
loss at step 1260: 0.318684
loss at step 1261: 0.333066
loss at step 1262: 0.321220
loss at step 1263: 0.319608
loss at step 1264: 0.319850
loss at step 1265: 0.322627
loss at step 1266: 0.322143
loss at step 1267: 0.325162
loss at step 1268: 0.318031
loss at step 1269: 0.321684
loss at step 1270: 0.323847
loss at step 1271: 0.315702
loss at step 1272: 0.332262
loss at step 1273: 0.317396
loss at step 1274: 0.321460
loss at step 1275: 0.320125
loss at step 1276: 0.321670
loss at step 1277: 0.323192
loss at step 1278: 0.325579
loss at step 1279: 0.322491
loss at step 1280: 0.320181
loss at step 1281: 0.325229
loss at step 1282: 0.316407
loss at step 1283: 0.342548
loss at step 1284: 0.336450
loss at step 1285: 0.338540
loss at step 1286: 0.326387
loss at step 1287: 0.318317
loss at step 1288: 0.317674
loss at step 1289: 0.320891
loss at step 1290: 0.321217
loss at step 1291: 0.328564
loss at step 1292: 0.320302
loss at step 1293: 0.319052
loss at step 1294: 0.316911
loss at step 1295: 0.324543
loss at step 1296: 0.321083
loss at step 1297: 0.338514
loss at step 1298: 0.338585
loss at step 1299: 0.339925
loss at step 1300: 0.338066
Average loss at step 1300: 0.325712, Accuracy = 0.0625
loss at step 1301: 0.340462
loss at step 1302: 0.336300
loss at step 1303: 0.319082
loss at step 1304: 0.321676
loss at step 1305: 0.318365
loss at step 1306: 0.324183
loss at step 1307: 0.326063
loss at step 1308: 0.327646
loss at step 1309: 0.318401
loss at step 1310: 0.319478
loss at step 1311: 0.322959
loss at step 1312: 0.314849
loss at step 1313: 0.330088
loss at step 1314: 0.319651
loss at step 1315: 0.320661
loss at step 1316: 0.320820
loss at step 1317: 0.320622
loss at step 1318: 0.324886
loss at step 1319: 0.327442
loss at step 1320: 0.322591
loss at step 1321: 0.323546
loss at step 1322: 0.322296
loss at step 1323: 0.317315
loss at step 1324: 0.334037
loss at step 1325: 0.324143
loss at step 1326: 0.320895
loss at step 1327: 0.324990
loss at step 1328: 0.319714
loss at step 1329: 0.335397
loss at step 1330: 0.337621
loss at step 1331: 0.338231
loss at step 1332: 0.333155
loss at step 1333: 0.319181
loss at step 1334: 0.319664
loss at step 1335: 0.317335
loss at step 1336: 0.325318
loss at step 1337: 0.323567
loss at step 1338: 0.326823
loss at step 1339: 0.319504
loss at step 1340: 0.318514
loss at step 1341: 0.324440
loss at step 1342: 0.314301
loss at step 1343: 0.337667
loss at step 1344: 0.335223
loss at step 1345: 0.341294
loss at step 1346: 0.338945
loss at step 1347: 0.339677
loss at step 1348: 0.337481
loss at step 1349: 0.325185
loss at step 1350: 0.322638
loss at step 1351: 0.320814
loss at step 1352: 0.323400
loss at step 1353: 0.320746
loss at step 1354: 0.335300
loss at step 1355: 0.318657
loss at step 1356: 0.320821
loss at step 1357: 0.325196
loss at step 1358: 0.316036
loss at step 1359: 0.330315
loss at step 1360: 0.317592
loss at step 1361: 0.319979
loss at step 1362: 0.319286
loss at step 1363: 0.323435
loss at step 1364: 0.318602
loss at step 1365: 0.330200
loss at step 1366: 0.321632
loss at step 1367: 0.320664
loss at step 1368: 0.322012
loss at step 1369: 0.318543
loss at step 1370: 0.332105
loss at step 1371: 0.320084
loss at step 1372: 0.320938
loss at step 1373: 0.318787
loss at step 1374: 0.324567
loss at step 1375: 0.330517
loss at step 1376: 0.337803
loss at step 1377: 0.338851
loss at step 1378: 0.337727
loss at step 1379: 0.321702
loss at step 1380: 0.320727
loss at step 1381: 0.320168
loss at step 1382: 0.325427
loss at step 1383: 0.319146
loss at step 1384: 0.333470
loss at step 1385: 0.319712
loss at step 1386: 0.319327
loss at step 1387: 0.321626
loss at step 1388: 0.320042
loss at step 1389: 0.330324
loss at step 1390: 0.335336
loss at step 1391: 0.339289
loss at step 1392: 0.340959
loss at step 1393: 0.338431
loss at step 1394: 0.337830
loss at step 1395: 0.329164
loss at step 1396: 0.323136
loss at step 1397: 0.319128
loss at step 1398: 0.324975
loss at step 1399: 0.318081
loss at step 1400: 0.337172
Average loss at step 1400: 0.325741, Accuracy = 0.14499999582767487
loss at step 1401: 0.316834
loss at step 1402: 0.318684
loss at step 1403: 0.322904
loss at step 1404: 0.321718
loss at step 1405: 0.323793
loss at step 1406: 0.321532
loss at step 1407: 0.317228
loss at step 1408: 0.320923
loss at step 1409: 0.324854
loss at step 1410: 0.314710
loss at step 1411: 0.331513
loss at step 1412: 0.318164
loss at step 1413: 0.321260
loss at step 1414: 0.318467
loss at step 1415: 0.320206
loss at step 1416: 0.326746
loss at step 1417: 0.323916
loss at step 1418: 0.320095
loss at step 1419: 0.318267
loss at step 1420: 0.323997
loss at step 1421: 0.321178
loss at step 1422: 0.341327
loss at step 1423: 0.336067
loss at step 1424: 0.337422
loss at step 1425: 0.324578
loss at step 1426: 0.317656
loss at step 1427: 0.317597
loss at step 1428: 0.323450
loss at step 1429: 0.317597
loss at step 1430: 0.332488
loss at step 1431: 0.317017
loss at step 1432: 0.319050
loss at step 1433: 0.315233
loss at step 1434: 0.322226
loss at step 1435: 0.323929
loss at step 1436: 0.336859
loss at step 1437: 0.337485
loss at step 1438: 0.338998
loss at step 1439: 0.337153
loss at step 1440: 0.338081
loss at step 1441: 0.333369
loss at step 1442: 0.320462
loss at step 1443: 0.320099
loss at step 1444: 0.322068
loss at step 1445: 0.322194
loss at step 1446: 0.329046
loss at step 1447: 0.322258
loss at step 1448: 0.320040
loss at step 1449: 0.320221
loss at step 1450: 0.323365
loss at step 1451: 0.318151
loss at step 1452: 0.327041
loss at step 1453: 0.319307
loss at step 1454: 0.321682
loss at step 1455: 0.323227
loss at step 1456: 0.318791
loss at step 1457: 0.327993
loss at step 1458: 0.321005
loss at step 1459: 0.319944
loss at step 1460: 0.320378
loss at step 1461: 0.321119
loss at step 1462: 0.320667
loss at step 1463: 0.328201
loss at step 1464: 0.320511
loss at step 1465: 0.317082
loss at step 1466: 0.326220
loss at step 1467: 0.317068
loss at step 1468: 0.338090
loss at step 1469: 0.335435
loss at step 1470: 0.336495
loss at step 1471: 0.328448
loss at step 1472: 0.316769
loss at step 1473: 0.316272
loss at step 1474: 0.315628
loss at step 1475: 0.323147
loss at step 1476: 0.324678
loss at step 1477: 0.322302
loss at step 1478: 0.317819
loss at step 1479: 0.316749
loss at step 1480: 0.323383
loss at step 1481: 0.312706
loss at step 1482: 0.341739
loss at step 1483: 0.335869
loss at step 1484: 0.341755
loss at step 1485: 0.339014
loss at step 1486: 0.341060
loss at step 1487: 0.340138
loss at step 1488: 0.328331
loss at step 1489: 0.325935
loss at step 1490: 0.321772
loss at step 1491: 0.326427
loss at step 1492: 0.326403
loss at step 1493: 0.334183
loss at step 1494: 0.319824
loss at step 1495: 0.321462
loss at step 1496: 0.326051
loss at step 1497: 0.317010
loss at step 1498: 0.330522
loss at step 1499: 0.317872
loss at step 1500: 0.318856
Average loss at step 1500: 0.324649, Accuracy = 0.10999999940395355
loss at step 1501: 0.319220
loss at step 1502: 0.322132
loss at step 1503: 0.320569
loss at step 1504: 0.323471
loss at step 1505: 0.319034
loss at step 1506: 0.317211
loss at step 1507: 0.322326
loss at step 1508: 0.315700
loss at step 1509: 0.329920
loss at step 1510: 0.319981
loss at step 1511: 0.319399
loss at step 1512: 0.321659
loss at step 1513: 0.321699
loss at step 1514: 0.331316
loss at step 1515: 0.336041
loss at step 1516: 0.336731
loss at step 1517: 0.333564
loss at step 1518: 0.317169
loss at step 1519: 0.318124
loss at step 1520: 0.316107
loss at step 1521: 0.322164
loss at step 1522: 0.319685
loss at step 1523: 0.328448
loss at step 1524: 0.319034
loss at step 1525: 0.316218
loss at step 1526: 0.323469
loss at step 1527: 0.316083
loss at step 1528: 0.334005
loss at step 1529: 0.334833
loss at step 1530: 0.337461
loss at step 1531: 0.342315
loss at step 1532: 0.339336
loss at step 1533: 0.336262
loss at step 1534: 0.326846
loss at step 1535: 0.322418
loss at step 1536: 0.319992
loss at step 1537: 0.321719
loss at step 1538: 0.316313
loss at step 1539: 0.336604
loss at step 1540: 0.317227
loss at step 1541: 0.316548
loss at step 1542: 0.323115
loss at step 1543: 0.315879
loss at step 1544: 0.325670
loss at step 1545: 0.314968
loss at step 1546: 0.318155
loss at step 1547: 0.313612
loss at step 1548: 0.320739
loss at step 1549: 0.311895
loss at step 1550: 0.327036
loss at step 1551: 0.315054
loss at step 1552: 0.317409
loss at step 1553: 0.315957
loss at step 1554: 0.315944
loss at step 1555: 0.326587
loss at step 1556: 0.317661
loss at step 1557: 0.317810
loss at step 1558: 0.314815
loss at step 1559: 0.321280
loss at step 1560: 0.324611
loss at step 1561: 0.335371
loss at step 1562: 0.335304
loss at step 1563: 0.333757
loss at step 1564: 0.318027
loss at step 1565: 0.314591
loss at step 1566: 0.314668
loss at step 1567: 0.321523
loss at step 1568: 0.312332
loss at step 1569: 0.330458
loss at step 1570: 0.314046
loss at step 1571: 0.315184
loss at step 1572: 0.315058
loss at step 1573: 0.319355
loss at step 1574: 0.324942
loss at step 1575: 0.330018
loss at step 1576: 0.333563
loss at step 1577: 0.337643
loss at step 1578: 0.333076
loss at step 1579: 0.333514
loss at step 1580: 0.325805
loss at step 1581: 0.317076
loss at step 1582: 0.312442
loss at step 1583: 0.318432
loss at step 1584: 0.312591
loss at step 1585: 0.327964
loss at step 1586: 0.313659
loss at step 1587: 0.313747
loss at step 1588: 0.313709
loss at step 1589: 0.314360
loss at step 1590: 0.315774
loss at step 1591: 0.316449
loss at step 1592: 0.311092
loss at step 1593: 0.313072
loss at step 1594: 0.313778
loss at step 1595: 0.304393
loss at step 1596: 0.323406
loss at step 1597: 0.307791
loss at step 1598: 0.313202
loss at step 1599: 0.308240
loss at step 1600: 0.308539
Average loss at step 1600: 0.321445, Accuracy = 0.16500000655651093
loss at step 1601: 0.313530
loss at step 1602: 0.312588
loss at step 1603: 0.309981
loss at step 1604: 0.308219
loss at step 1605: 0.309910
loss at step 1606: 0.302274
loss at step 1607: 0.335661
loss at step 1608: 0.322414
loss at step 1609: 0.322786
loss at step 1610: 0.314259
loss at step 1611: 0.305643
loss at step 1612: 0.300750
loss at step 1613: 0.307650
loss at step 1614: 0.303320
loss at step 1615: 0.315596
loss at step 1616: 0.308844
loss at step 1617: 0.305461
loss at step 1618: 0.301843
loss at step 1619: 0.302673
loss at step 1620: 0.304548
loss at step 1621: 0.330823
loss at step 1622: 0.319600
loss at step 1623: 0.328489
loss at step 1624: 0.334424
loss at step 1625: 0.326502
loss at step 1626: 0.326505
loss at step 1627: 0.310110
loss at step 1628: 0.307276
loss at step 1629: 0.302202
loss at step 1630: 0.301009
loss at step 1631: 0.305932
loss at step 1632: 0.313584
loss at step 1633: 0.306213
loss at step 1634: 0.301357
loss at step 1635: 0.302039
loss at step 1636: 0.295474
loss at step 1637: 0.314336
loss at step 1638: 0.301015
loss at step 1639: 0.303164
loss at step 1640: 0.297402
loss at step 1641: 0.293680
loss at step 1642: 0.304273
loss at step 1643: 0.304302
loss at step 1644: 0.300608
loss at step 1645: 0.300492
loss at step 1646: 0.293591
loss at step 1647: 0.291988
loss at step 1648: 0.312110
loss at step 1649: 0.302514
loss at step 1650: 0.299768
loss at step 1651: 0.300308
loss at step 1652: 0.294806
loss at step 1653: 0.317045
loss at step 1654: 0.315422
loss at step 1655: 0.311727
loss at step 1656: 0.307143
loss at step 1657: 0.296005
loss at step 1658: 0.293628
loss at step 1659: 0.292633
loss at step 1660: 0.299541
loss at step 1661: 0.300342
loss at step 1662: 0.306538
loss at step 1663: 0.296301
loss at step 1664: 0.299602
loss at step 1665: 0.300672
loss at step 1666: 0.291116
loss at step 1667: 0.312657
loss at step 1668: 0.311608
loss at step 1669: 0.318659
loss at step 1670: 0.320116
loss at step 1671: 0.317099
loss at step 1672: 0.316866
loss at step 1673: 0.298258
loss at step 1674: 0.294691
loss at step 1675: 0.292756
loss at step 1676: 0.295282
loss at step 1677: 0.292205
loss at step 1678: 0.312928
loss at step 1679: 0.294751
loss at step 1680: 0.293670
loss at step 1681: 0.293000
loss at step 1682: 0.286951
loss at step 1683: 0.303394
loss at step 1684: 0.292596
loss at step 1685: 0.294412
loss at step 1686: 0.292134
loss at step 1687: 0.288898
loss at step 1688: 0.286929
loss at step 1689: 0.302652
loss at step 1690: 0.296847
loss at step 1691: 0.290988
loss at step 1692: 0.285400
loss at step 1693: 0.280135
loss at step 1694: 0.300296
loss at step 1695: 0.293180
loss at step 1696: 0.292801
loss at step 1697: 0.288534
loss at step 1698: 0.288306
loss at step 1699: 0.299371
loss at step 1700: 0.307943
Average loss at step 1700: 0.303979, Accuracy = 0.29249998927116394
loss at step 1701: 0.307797
loss at step 1702: 0.305404
loss at step 1703: 0.287913
loss at step 1704: 0.291146
loss at step 1705: 0.290143
loss at step 1706: 0.288538
loss at step 1707: 0.286754
loss at step 1708: 0.304162
loss at step 1709: 0.288578
loss at step 1710: 0.291406
loss at step 1711: 0.287686
loss at step 1712: 0.285055
loss at step 1713: 0.290919
loss at step 1714: 0.301732
loss at step 1715: 0.300722
loss at step 1716: 0.311121
loss at step 1717: 0.308248
loss at step 1718: 0.300847
loss at step 1719: 0.284691
loss at step 1720: 0.283272
loss at step 1721: 0.282591
loss at step 1722: 0.280312
loss at step 1723: 0.273647
loss at step 1724: 0.303793
loss at step 1725: 0.285886
loss at step 1726: 0.280504
loss at step 1727: 0.275570
loss at step 1728: 0.274230
loss at step 1729: 0.281303
loss at step 1730: 0.285030
loss at step 1731: 0.283630
loss at step 1732: 0.273855
loss at step 1733: 0.271612
loss at step 1734: 0.260738
loss at step 1735: 0.295160
loss at step 1736: 0.277844
loss at step 1737: 0.287282
loss at step 1738: 0.264751
loss at step 1739: 0.258492
loss at step 1740: 0.278585
loss at step 1741: 0.273988
loss at step 1742: 0.274451
loss at step 1743: 0.274079
loss at step 1744: 0.272902
loss at step 1745: 0.276629
loss at step 1746: 0.299358
loss at step 1747: 0.298170
loss at step 1748: 0.296567
loss at step 1749: 0.282079
loss at step 1750: 0.276078
loss at step 1751: 0.272876
loss at step 1752: 0.268852
loss at step 1753: 0.261382
loss at step 1754: 0.299491
loss at step 1755: 0.278521
loss at step 1756: 0.276613
loss at step 1757: 0.264306
loss at step 1758: 0.267528
loss at step 1759: 0.271401
loss at step 1760: 0.289324
loss at step 1761: 0.289400
loss at step 1762: 0.294895
loss at step 1763: 0.298465
loss at step 1764: 0.286490
loss at step 1765: 0.277106
loss at step 1766: 0.262274
loss at step 1767: 0.270373
loss at step 1768: 0.268821
loss at step 1769: 0.266056
loss at step 1770: 0.272202
loss at step 1771: 0.264766
loss at step 1772: 0.260094
loss at step 1773: 0.257664
loss at step 1774: 0.259652
loss at step 1775: 0.253992
loss at step 1776: 0.263396
loss at step 1777: 0.255774
loss at step 1778: 0.266349
loss at step 1779: 0.248873
loss at step 1780: 0.245768
loss at step 1781: 0.274236
loss at step 1782: 0.256197
loss at step 1783: 0.260414
loss at step 1784: 0.251917
loss at step 1785: 0.242140
loss at step 1786: 0.255177
loss at step 1787: 0.273913
loss at step 1788: 0.263964
loss at step 1789: 0.263573
loss at step 1790: 0.252162
loss at step 1791: 0.242435
loss at step 1792: 0.308124
loss at step 1793: 0.294804
loss at step 1794: 0.283465
loss at step 1795: 0.278936
loss at step 1796: 0.270967
loss at step 1797: 0.263660
loss at step 1798: 0.252129
loss at step 1799: 0.253349
loss at step 1800: 0.273973
Average loss at step 1800: 0.277235, Accuracy = 0.3199999928474426
loss at step 1801: 0.279353
loss at step 1802: 0.272851
loss at step 1803: 0.255881
loss at step 1804: 0.257506
loss at step 1805: 0.242551
loss at step 1806: 0.282602
loss at step 1807: 0.279046
loss at step 1808: 0.284990
loss at step 1809: 0.284080
loss at step 1810: 0.276195
loss at step 1811: 0.274027
loss at step 1812: 0.249828
loss at step 1813: 0.253870
loss at step 1814: 0.253889
loss at step 1815: 0.255883
loss at step 1816: 0.246778
loss at step 1817: 0.263320
loss at step 1818: 0.249396
loss at step 1819: 0.244129
loss at step 1820: 0.247080
loss at step 1821: 0.230712
loss at step 1822: 0.259465
loss at step 1823: 0.243692
loss at step 1824: 0.251113
loss at step 1825: 0.242245
loss at step 1826: 0.241643
loss at step 1827: 0.249667
loss at step 1828: 0.255601
loss at step 1829: 0.252589
loss at step 1830: 0.252949
loss at step 1831: 0.235465
loss at step 1832: 0.231085
loss at step 1833: 0.268569
loss at step 1834: 0.253830
loss at step 1835: 0.246205
loss at step 1836: 0.237613
loss at step 1837: 0.237473
loss at step 1838: 0.262884
loss at step 1839: 0.260646
loss at step 1840: 0.256270
loss at step 1841: 0.260649
loss at step 1842: 0.242695
loss at step 1843: 0.248994
loss at step 1844: 0.237422
loss at step 1845: 0.233328
loss at step 1846: 0.224000
loss at step 1847: 0.259884
loss at step 1848: 0.243198
loss at step 1849: 0.225653
loss at step 1850: 0.225882
loss at step 1851: 0.206167
loss at step 1852: 0.255500
loss at step 1853: 0.255541
loss at step 1854: 0.260646
loss at step 1855: 0.272383
loss at step 1856: 0.266155
loss at step 1857: 0.256025
loss at step 1858: 0.231560
loss at step 1859: 0.230768
loss at step 1860: 0.229202
loss at step 1861: 0.235908
loss at step 1862: 0.218991
loss at step 1863: 0.255071
loss at step 1864: 0.232700
loss at step 1865: 0.228248
loss at step 1866: 0.231640
loss at step 1867: 0.214717
loss at step 1868: 0.239059
loss at step 1869: 0.218785
loss at step 1870: 0.234388
loss at step 1871: 0.222495
loss at step 1872: 0.226862
loss at step 1873: 0.217162
loss at step 1874: 0.244562
loss at step 1875: 0.229539
loss at step 1876: 0.229396
loss at step 1877: 0.214700
loss at step 1878: 0.208920
loss at step 1879: 0.252024
loss at step 1880: 0.233720
loss at step 1881: 0.242045
loss at step 1882: 0.228459
loss at step 1883: 0.231017
loss at step 1884: 0.243293
loss at step 1885: 0.259830
loss at step 1886: 0.250648
loss at step 1887: 0.247835
loss at step 1888: 0.217752
loss at step 1889: 0.225066
loss at step 1890: 0.216273
loss at step 1891: 0.218048
loss at step 1892: 0.201017
loss at step 1893: 0.265928
loss at step 1894: 0.242749
loss at step 1895: 0.227133
loss at step 1896: 0.207050
loss at step 1897: 0.197272
loss at step 1898: 0.231532
loss at step 1899: 0.259059
loss at step 1900: 0.249976
Average loss at step 1900: 0.243395, Accuracy = 0.4650000035762787
loss at step 1901: 0.259528
loss at step 1902: 0.259611
loss at step 1903: 0.243383
loss at step 1904: 0.230055
loss at step 1905: 0.216156
loss at step 1906: 0.215801
loss at step 1907: 0.230855
loss at step 1908: 0.206129
loss at step 1909: 0.234294
loss at step 1910: 0.226315
loss at step 1911: 0.217678
loss at step 1912: 0.214064
loss at step 1913: 0.212784
loss at step 1914: 0.216771
loss at step 1915: 0.230897
loss at step 1916: 0.220772
loss at step 1917: 0.220197
loss at step 1918: 0.210303
loss at step 1919: 0.193092
loss at step 1920: 0.233296
loss at step 1921: 0.205209
loss at step 1922: 0.210587
loss at step 1923: 0.195216
loss at step 1924: 0.192781
loss at step 1925: 0.202574
loss at step 1926: 0.209262
loss at step 1927: 0.201397
loss at step 1928: 0.205069
loss at step 1929: 0.199269
loss at step 1930: 0.199098
loss at step 1931: 0.241494
loss at step 1932: 0.219091
loss at step 1933: 0.220524
loss at step 1934: 0.220518
loss at step 1935: 0.208792
loss at step 1936: 0.201894
loss at step 1937: 0.203068
loss at step 1938: 0.209232
loss at step 1939: 0.245928
loss at step 1940: 0.232323
loss at step 1941: 0.234091
loss at step 1942: 0.190973
loss at step 1943: 0.193825
loss at step 1944: 0.196547
loss at step 1945: 0.265571
loss at step 1946: 0.254566
loss at step 1947: 0.242031
loss at step 1948: 0.260543
loss at step 1949: 0.239709
loss at step 1950: 0.251111
loss at step 1951: 0.221186
loss at step 1952: 0.221755
loss at step 1953: 0.219756
loss at step 1954: 0.206842
loss at step 1955: 0.198114
loss at step 1956: 0.219813
loss at step 1957: 0.208593
loss at step 1958: 0.197279
loss at step 1959: 0.196284
loss at step 1960: 0.175728
loss at step 1961: 0.225541
loss at step 1962: 0.211716
loss at step 1963: 0.207766
loss at step 1964: 0.195043
loss at step 1965: 0.192263
loss at step 1966: 0.205181
loss at step 1967: 0.203764
loss at step 1968: 0.202524
loss at step 1969: 0.201200
loss at step 1970: 0.184447
loss at step 1971: 0.180897
loss at step 1972: 0.217005
loss at step 1973: 0.200754
loss at step 1974: 0.203124
loss at step 1975: 0.197421
loss at step 1976: 0.193288
loss at step 1977: 0.228008
loss at step 1978: 0.215847
loss at step 1979: 0.212053
loss at step 1980: 0.214489
loss at step 1981: 0.188902
loss at step 1982: 0.185656
loss at step 1983: 0.184009
loss at step 1984: 0.197202
loss at step 1985: 0.201286
loss at step 1986: 0.200578
loss at step 1987: 0.195462
loss at step 1988: 0.163217
loss at step 1989: 0.172067
loss at step 1990: 0.145972
loss at step 1991: 0.240364
loss at step 1992: 0.226727
loss at step 1993: 0.238710
loss at step 1994: 0.260189
loss at step 1995: 0.243594
loss at step 1996: 0.250242
loss at step 1997: 0.233495
loss at step 1998: 0.235981
loss at step 1999: 0.215059
loss at step 2000: 0.221011
Average loss at step 2000: 0.214017, Accuracy = 0.44999998807907104
loss at step 2001: 0.185376
loss at step 2002: 0.216833
loss at step 2003: 0.209075
loss at step 2004: 0.200720
loss at step 2005: 0.200721
loss at step 2006: 0.167972
loss at step 2007: 0.196080
loss at step 2008: 0.172856
loss at step 2009: 0.180032
loss at step 2010: 0.180807
loss at step 2011: 0.183059
loss at step 2012: 0.166619
loss at step 2013: 0.198028
loss at step 2014: 0.193964
loss at step 2015: 0.189738
loss at step 2016: 0.182828
loss at step 2017: 0.170379
loss at step 2018: 0.211524
loss at step 2019: 0.179049
loss at step 2020: 0.180166
loss at step 2021: 0.183628
loss at step 2022: 0.182950
loss at step 2023: 0.209263
loss at step 2024: 0.227853
loss at step 2025: 0.224979
loss at step 2026: 0.218053
loss at step 2027: 0.190167
loss at step 2028: 0.197593
loss at step 2029: 0.187952
loss at step 2030: 0.200373
loss at step 2031: 0.191907
loss at step 2032: 0.233142
loss at step 2033: 0.221581
loss at step 2034: 0.208271
loss at step 2035: 0.200091
loss at step 2036: 0.190049
loss at step 2037: 0.195622
loss at step 2038: 0.187008
loss at step 2039: 0.192266
loss at step 2040: 0.214637
loss at step 2041: 0.206969
loss at step 2042: 0.200823
loss at step 2043: 0.209173
loss at step 2044: 0.204356
loss at step 2045: 0.181163
loss at step 2046: 0.197078
loss at step 2047: 0.178752
loss at step 2048: 0.227136
loss at step 2049: 0.210939
loss at step 2050: 0.189750
loss at step 2051: 0.186036
loss at step 2052: 0.166162
loss at step 2053: 0.204941
loss at step 2054: 0.230319
loss at step 2055: 0.219596
loss at step 2056: 0.213451
loss at step 2057: 0.215804
loss at step 2058: 0.184662
loss at step 2059: 0.212467
loss at step 2060: 0.189356
loss at step 2061: 0.189562
loss at step 2062: 0.201847
loss at step 2063: 0.163540
loss at step 2064: 0.183248
loss at step 2065: 0.182721
loss at step 2066: 0.166963
loss at step 2067: 0.189893
loss at step 2068: 0.179110
loss at step 2069: 0.172319
loss at step 2070: 0.208387
loss at step 2071: 0.200590
loss at step 2072: 0.189524
loss at step 2073: 0.176881
loss at step 2074: 0.171221
loss at step 2075: 0.171197
loss at step 2076: 0.178604
loss at step 2077: 0.160810
loss at step 2078: 0.206119
loss at step 2079: 0.187458
loss at step 2080: 0.187921
loss at step 2081: 0.169571
loss at step 2082: 0.163894
loss at step 2083: 0.168746
loss at step 2084: 0.197549
loss at step 2085: 0.190848
loss at step 2086: 0.201509
loss at step 2087: 0.218916
loss at step 2088: 0.195510
loss at step 2089: 0.208618
loss at step 2090: 0.174334
loss at step 2091: 0.177466
loss at step 2092: 0.177453
loss at step 2093: 0.174426
loss at step 2094: 0.183885
loss at step 2095: 0.189364
loss at step 2096: 0.174714
loss at step 2097: 0.162682
loss at step 2098: 0.168083
loss at step 2099: 0.147864
loss at step 2100: 0.185697
Average loss at step 2100: 0.191512, Accuracy = 0.6100000143051147
loss at step 2101: 0.149974
loss at step 2102: 0.147156
loss at step 2103: 0.137826
loss at step 2104: 0.143675
loss at step 2105: 0.167155
loss at step 2106: 0.143187
loss at step 2107: 0.135680
loss at step 2108: 0.147177
loss at step 2109: 0.132702
loss at step 2110: 0.140113
loss at step 2111: 0.171211
loss at step 2112: 0.137973
loss at step 2113: 0.152648
loss at step 2114: 0.159180
loss at step 2115: 0.154318
loss at step 2116: 0.182551
loss at step 2117: 0.167449
loss at step 2118: 0.162548
loss at step 2119: 0.188910
loss at step 2120: 0.161644
loss at step 2121: 0.145820
loss at step 2122: 0.153368
loss at step 2123: 0.161807
loss at step 2124: 0.159135
loss at step 2125: 0.162967
loss at step 2126: 0.172269
loss at step 2127: 0.157874
loss at step 2128: 0.150913
loss at step 2129: 0.120656
loss at step 2130: 0.234499
loss at step 2131: 0.205569
loss at step 2132: 0.204052
loss at step 2133: 0.223357
loss at step 2134: 0.195911
loss at step 2135: 0.187905
loss at step 2136: 0.165627
loss at step 2137: 0.173832
loss at step 2138: 0.160821
loss at step 2139: 0.170082
loss at step 2140: 0.138568
loss at step 2141: 0.182402
loss at step 2142: 0.167161
loss at step 2143: 0.157417
loss at step 2144: 0.152325
loss at step 2145: 0.120570
loss at step 2146: 0.191457
loss at step 2147: 0.155280
loss at step 2148: 0.139001
loss at step 2149: 0.123927
loss at step 2150: 0.145877
loss at step 2151: 0.154603
loss at step 2152: 0.157236
loss at step 2153: 0.144925
loss at step 2154: 0.132162
loss at step 2155: 0.127567
loss at step 2156: 0.121738
loss at step 2157: 0.177929
loss at step 2158: 0.133398
loss at step 2159: 0.119057
loss at step 2160: 0.143026
loss at step 2161: 0.156950
loss at step 2162: 0.146177
loss at step 2163: 0.131022
loss at step 2164: 0.131199
loss at step 2165: 0.164248
loss at step 2166: 0.142447
loss at step 2167: 0.146107
loss at step 2168: 0.138583
loss at step 2169: 0.166915
loss at step 2170: 0.148493
loss at step 2171: 0.172601
loss at step 2172: 0.162359
loss at step 2173: 0.147736
loss at step 2174: 0.134266
loss at step 2175: 0.104993
loss at step 2176: 0.162197
loss at step 2177: 0.175003
loss at step 2178: 0.177370
loss at step 2179: 0.230834
loss at step 2180: 0.219594
loss at step 2181: 0.189039
loss at step 2182: 0.190032
loss at step 2183: 0.174267
loss at step 2184: 0.175706
loss at step 2185: 0.169018
loss at step 2186: 0.137900
loss at step 2187: 0.205395
loss at step 2188: 0.182758
loss at step 2189: 0.170659
loss at step 2190: 0.168131
loss at step 2191: 0.132126
loss at step 2192: 0.177819
loss at step 2193: 0.171899
loss at step 2194: 0.160495
loss at step 2195: 0.136111
loss at step 2196: 0.155387
loss at step 2197: 0.132310
loss at step 2198: 0.172926
loss at step 2199: 0.155758
loss at step 2200: 0.141164
Average loss at step 2200: 0.159332, Accuracy = 0.6274999976158142
loss at step 2201: 0.136641
loss at step 2202: 0.124223
loss at step 2203: 0.179292
loss at step 2204: 0.146188
loss at step 2205: 0.139619
loss at step 2206: 0.135447
loss at step 2207: 0.164123
loss at step 2208: 0.176199
loss at step 2209: 0.178697
loss at step 2210: 0.164071
loss at step 2211: 0.184579
loss at step 2212: 0.174426
loss at step 2213: 0.168895
loss at step 2214: 0.140848
loss at step 2215: 0.168610
loss at step 2216: 0.159590
loss at step 2217: 0.253794
loss at step 2218: 0.217437
loss at step 2219: 0.205325
loss at step 2220: 0.181609
loss at step 2221: 0.172914
loss at step 2222: 0.183603
loss at step 2223: 0.207718
loss at step 2224: 0.198809
loss at step 2225: 0.245189
loss at step 2226: 0.242046
loss at step 2227: 0.229027
loss at step 2228: 0.216394
loss at step 2229: 0.183350
loss at step 2230: 0.164970
loss at step 2231: 0.167228
loss at step 2232: 0.148228
loss at step 2233: 0.173847
loss at step 2234: 0.170274
loss at step 2235: 0.164132
loss at step 2236: 0.156145
loss at step 2237: 0.138862
loss at step 2238: 0.131199
loss at step 2239: 0.152645
loss at step 2240: 0.124050
loss at step 2241: 0.143030
loss at step 2242: 0.132438
loss at step 2243: 0.105149
loss at step 2244: 0.178716
loss at step 2245: 0.172492
loss at step 2246: 0.155064
loss at step 2247: 0.183537
loss at step 2248: 0.155988
loss at step 2249: 0.180047
loss at step 2250: 0.203594
loss at step 2251: 0.164930
loss at step 2252: 0.176048
loss at step 2253: 0.181963
loss at step 2254: 0.168571
loss at step 2255: 0.230315
loss at step 2256: 0.199426
loss at step 2257: 0.199041
loss at step 2258: 0.166283
loss at step 2259: 0.141767
loss at step 2260: 0.127537
loss at step 2261: 0.149128
loss at step 2262: 0.165316
loss at step 2263: 0.170208
loss at step 2264: 0.145903
loss at step 2265: 0.142508
loss at step 2266: 0.116217
loss at step 2267: 0.145109
loss at step 2268: 0.133237
loss at step 2269: 0.188265
loss at step 2270: 0.182059
loss at step 2271: 0.196096
loss at step 2272: 0.203924
loss at step 2273: 0.184214
loss at step 2274: 0.188928
loss at step 2275: 0.149033
loss at step 2276: 0.147973
loss at step 2277: 0.140261
loss at step 2278: 0.151714
loss at step 2279: 0.160819
loss at step 2280: 0.191217
loss at step 2281: 0.190072
loss at step 2282: 0.160627
loss at step 2283: 0.162838
loss at step 2284: 0.130026
loss at step 2285: 0.230825
loss at step 2286: 0.202835
loss at step 2287: 0.177703
loss at step 2288: 0.163923
loss at step 2289: 0.149960
loss at step 2290: 0.148529
loss at step 2291: 0.142745
loss at step 2292: 0.126080
loss at step 2293: 0.138410
loss at step 2294: 0.125649
loss at step 2295: 0.116107
loss at step 2296: 0.175624
loss at step 2297: 0.136398
loss at step 2298: 0.160982
loss at step 2299: 0.159533
loss at step 2300: 0.138908
Average loss at step 2300: 0.167261, Accuracy = 0.675000011920929
loss at step 2301: 0.213784
loss at step 2302: 0.210141
loss at step 2303: 0.193194
loss at step 2304: 0.199716
loss at step 2305: 0.160097
loss at step 2306: 0.154623
loss at step 2307: 0.151226
loss at step 2308: 0.182846
loss at step 2309: 0.173612
loss at step 2310: 0.184880
loss at step 2311: 0.165391
loss at step 2312: 0.129008
loss at step 2313: 0.137812
loss at step 2314: 0.103827
loss at step 2315: 0.174469
loss at step 2316: 0.138708
loss at step 2317: 0.170228
loss at step 2318: 0.181686
loss at step 2319: 0.165189
loss at step 2320: 0.152810
loss at step 2321: 0.125017
loss at step 2322: 0.122441
loss at step 2323: 0.113610
loss at step 2324: 0.129306
loss at step 2325: 0.107781
loss at step 2326: 0.185422
loss at step 2327: 0.165620
loss at step 2328: 0.129998
loss at step 2329: 0.134868
loss at step 2330: 0.103268
loss at step 2331: 0.198741
loss at step 2332: 0.183367
loss at step 2333: 0.174263
loss at step 2334: 0.145474
loss at step 2335: 0.171189
loss at step 2336: 0.138860
loss at step 2337: 0.147240
loss at step 2338: 0.136624
loss at step 2339: 0.126386
loss at step 2340: 0.129374
loss at step 2341: 0.114839
loss at step 2342: 0.160631
loss at step 2343: 0.137348
loss at step 2344: 0.124258
loss at step 2345: 0.137937
loss at step 2346: 0.149655
loss at step 2347: 0.139618
loss at step 2348: 0.138850
loss at step 2349: 0.132061
loss at step 2350: 0.159184
loss at step 2351: 0.137994
loss at step 2352: 0.131417
loss at step 2353: 0.132211
loss at step 2354: 0.153011
loss at step 2355: 0.142408
loss at step 2356: 0.222537
loss at step 2357: 0.207114
loss at step 2358: 0.165386
loss at step 2359: 0.159240
loss at step 2360: 0.133776
loss at step 2361: 0.190201
loss at step 2362: 0.201124
loss at step 2363: 0.188317
loss at step 2364: 0.228736
loss at step 2365: 0.209000
loss at step 2366: 0.188086
loss at step 2367: 0.163550
loss at step 2368: 0.140146
loss at step 2369: 0.123276
loss at step 2370: 0.139851
loss at step 2371: 0.120433
loss at step 2372: 0.151974
loss at step 2373: 0.131339
loss at step 2374: 0.119952
loss at step 2375: 0.123510
loss at step 2376: 0.115110
loss at step 2377: 0.145109
loss at step 2378: 0.168902
loss at step 2379: 0.161726
loss at step 2380: 0.139856
loss at step 2381: 0.161893
loss at step 2382: 0.108056
loss at step 2383: 0.180394
loss at step 2384: 0.156559
loss at step 2385: 0.162745
loss at step 2386: 0.165104
loss at step 2387: 0.147554
loss at step 2388: 0.161470
loss at step 2389: 0.143121
loss at step 2390: 0.123516
loss at step 2391: 0.147102
loss at step 2392: 0.152853
loss at step 2393: 0.155172
loss at step 2394: 0.231668
loss at step 2395: 0.214268
loss at step 2396: 0.195613
loss at step 2397: 0.151087
loss at step 2398: 0.121428
loss at step 2399: 0.123701
loss at step 2400: 0.147785
Average loss at step 2400: 0.154929, Accuracy = 0.925000011920929
loss at step 2401: 0.133843
loss at step 2402: 0.154233
loss at step 2403: 0.130616
loss at step 2404: 0.106004
loss at step 2405: 0.089712
loss at step 2406: 0.108064
loss at step 2407: 0.129830
loss at step 2408: 0.182892
loss at step 2409: 0.175899
loss at step 2410: 0.181649
loss at step 2411: 0.178720
loss at step 2412: 0.154568
loss at step 2413: 0.155638
loss at step 2414: 0.130343
loss at step 2415: 0.107998
loss at step 2416: 0.112040
loss at step 2417: 0.107313
loss at step 2418: 0.127424
loss at step 2419: 0.144295
loss at step 2420: 0.123752
loss at step 2421: 0.097876
loss at step 2422: 0.114697
loss at step 2423: 0.097076
loss at step 2424: 0.161822
loss at step 2425: 0.145752
loss at step 2426: 0.137472
loss at step 2427: 0.132687
loss at step 2428: 0.103476
loss at step 2429: 0.129709
loss at step 2430: 0.101852
loss at step 2431: 0.108822
loss at step 2432: 0.109612
loss at step 2433: 0.118422
loss at step 2434: 0.111199
loss at step 2435: 0.121045
loss at step 2436: 0.093054
loss at step 2437: 0.111151
loss at step 2438: 0.116693
loss at step 2439: 0.111113
loss at step 2440: 0.209721
loss at step 2441: 0.184180
loss at step 2442: 0.191205
loss at step 2443: 0.178951
loss at step 2444: 0.140506
loss at step 2445: 0.129553
loss at step 2446: 0.145880
loss at step 2447: 0.158241
loss at step 2448: 0.150708
loss at step 2449: 0.155478
loss at step 2450: 0.122125
loss at step 2451: 0.110833
loss at step 2452: 0.119110
loss at step 2453: 0.091424
loss at step 2454: 0.137543
loss at step 2455: 0.126270
loss at step 2456: 0.134669
loss at step 2457: 0.149057
loss at step 2458: 0.129108
loss at step 2459: 0.132058
loss at step 2460: 0.113175
loss at step 2461: 0.104381
loss at step 2462: 0.108083
loss at step 2463: 0.115887
loss at step 2464: 0.125222
loss at step 2465: 0.185819
loss at step 2466: 0.146777
loss at step 2467: 0.110260
loss at step 2468: 0.114609
loss at step 2469: 0.077907
loss at step 2470: 0.170328
loss at step 2471: 0.154727
loss at step 2472: 0.141661
loss at step 2473: 0.134338
loss at step 2474: 0.135459
loss at step 2475: 0.123524
loss at step 2476: 0.126604
loss at step 2477: 0.108272
loss at step 2478: 0.109444
loss at step 2479: 0.104716
loss at step 2480: 0.093201
loss at step 2481: 0.128212
loss at step 2482: 0.095337
loss at step 2483: 0.092838
loss at step 2484: 0.101160
loss at step 2485: 0.107852
loss at step 2486: 0.137061
loss at step 2487: 0.136268
loss at step 2488: 0.137229
loss at step 2489: 0.147406
loss at step 2490: 0.115896
loss at step 2491: 0.131059
loss at step 2492: 0.122224
loss at step 2493: 0.158643
loss at step 2494: 0.152448
loss at step 2495: 0.197908
loss at step 2496: 0.187677
loss at step 2497: 0.160531
loss at step 2498: 0.163596
loss at step 2499: 0.124366
loss at step 2500: 0.175767
Average loss at step 2500: 0.132329, Accuracy = 0.6650000214576721
loss at step 2501: 0.157454
loss at step 2502: 0.169889
loss at step 2503: 0.201215
loss at step 2504: 0.182702
loss at step 2505: 0.162478
loss at step 2506: 0.125722
loss at step 2507: 0.106947
loss at step 2508: 0.102687
loss at step 2509: 0.109893
loss at step 2510: 0.088956
loss at step 2511: 0.126067
loss at step 2512: 0.115616
loss at step 2513: 0.087376
loss at step 2514: 0.100042
loss at step 2515: 0.082256
loss at step 2516: 0.113366
loss at step 2517: 0.111685
loss at step 2518: 0.099532
loss at step 2519: 0.096056
loss at step 2520: 0.114399
loss at step 2521: 0.079519
loss at step 2522: 0.146075
loss at step 2523: 0.130924
loss at step 2524: 0.121441
loss at step 2525: 0.125437
loss at step 2526: 0.107672
loss at step 2527: 0.128135
loss at step 2528: 0.100181
loss at step 2529: 0.088401
loss at step 2530: 0.086444
loss at step 2531: 0.104207
loss at step 2532: 0.138707
loss at step 2533: 0.203205
loss at step 2534: 0.175212
loss at step 2535: 0.166238
loss at step 2536: 0.104990
loss at step 2537: 0.106656
loss at step 2538: 0.095093
loss at step 2539: 0.126716
loss at step 2540: 0.124583
loss at step 2541: 0.171509
loss at step 2542: 0.149948
loss at step 2543: 0.126089
loss at step 2544: 0.120019
loss at step 2545: 0.120479
loss at step 2546: 0.132473
loss at step 2547: 0.153340
loss at step 2548: 0.152631
loss at step 2549: 0.163793
loss at step 2550: 0.151796
loss at step 2551: 0.133809
loss at step 2552: 0.121112
loss at step 2553: 0.099115
loss at step 2554: 0.081296
loss at step 2555: 0.097924
loss at step 2556: 0.073892
loss at step 2557: 0.106938
loss at step 2558: 0.098105
loss at step 2559: 0.068503
loss at step 2560: 0.075227
loss at step 2561: 0.080119
loss at step 2562: 0.069155
loss at step 2563: 0.087969
loss at step 2564: 0.064354
loss at step 2565: 0.070905
loss at step 2566: 0.076782
loss at step 2567: 0.037316
loss at step 2568: 0.092516
loss at step 2569: 0.076253
loss at step 2570: 0.066690
loss at step 2571: 0.081453
loss at step 2572: 0.072817
loss at step 2573: 0.097005
loss at step 2574: 0.105967
loss at step 2575: 0.063985
loss at step 2576: 0.076384
loss at step 2577: 0.078391
loss at step 2578: 0.090089
loss at step 2579: 0.198059
loss at step 2580: 0.166758
loss at step 2581: 0.163412
loss at step 2582: 0.111883
loss at step 2583: 0.091361
loss at step 2584: 0.081342
loss at step 2585: 0.104131
loss at step 2586: 0.113032
loss at step 2587: 0.105176
loss at step 2588: 0.085996
loss at step 2589: 0.077366
loss at step 2590: 0.061183
loss at step 2591: 0.082272
loss at step 2592: 0.070859
loss at step 2593: 0.134760
loss at step 2594: 0.133062
loss at step 2595: 0.134891
loss at step 2596: 0.141431
loss at step 2597: 0.116373
loss at step 2598: 0.122988
loss at step 2599: 0.099562
loss at step 2600: 0.077344
Average loss at step 2600: 0.111736, Accuracy = 0.9375
loss at step 2601: 0.087791
loss at step 2602: 0.077261
loss at step 2603: 0.092268
loss at step 2604: 0.115739
loss at step 2605: 0.099538
loss at step 2606: 0.082444
loss at step 2607: 0.088243
loss at step 2608: 0.056330
loss at step 2609: 0.100628
loss at step 2610: 0.080408
loss at step 2611: 0.076260
loss at step 2612: 0.077661
loss at step 2613: 0.076042
loss at step 2614: 0.084214
loss at step 2615: 0.083655
loss at step 2616: 0.074833
loss at step 2617: 0.081056
loss at step 2618: 0.080957
loss at step 2619: 0.077577
loss at step 2620: 0.093732
loss at step 2621: 0.064250
loss at step 2622: 0.073907
loss at step 2623: 0.069295
loss at step 2624: 0.067951
loss at step 2625: 0.211592
loss at step 2626: 0.193690
loss at step 2627: 0.177092
loss at step 2628: 0.137866
loss at step 2629: 0.087704
loss at step 2630: 0.090225
loss at step 2631: 0.091862
loss at step 2632: 0.137075
loss at step 2633: 0.115058
loss at step 2634: 0.109987
loss at step 2635: 0.096321
loss at step 2636: 0.078119
loss at step 2637: 0.089176
loss at step 2638: 0.054000
loss at step 2639: 0.139194
loss at step 2640: 0.120753
loss at step 2641: 0.128793
loss at step 2642: 0.129892
loss at step 2643: 0.120771
loss at step 2644: 0.103479
loss at step 2645: 0.082365
loss at step 2646: 0.063257
loss at step 2647: 0.064064
loss at step 2648: 0.073919
loss at step 2649: 0.061106
loss at step 2650: 0.097483
loss at step 2651: 0.086123
loss at step 2652: 0.064831
loss at step 2653: 0.071181
loss at step 2654: 0.044668
loss at step 2655: 0.078014
loss at step 2656: 0.071810
loss at step 2657: 0.052320
loss at step 2658: 0.059119
loss at step 2659: 0.065544
loss at step 2660: 0.054085
loss at step 2661: 0.075380
loss at step 2662: 0.061204
loss at step 2663: 0.061601
loss at step 2664: 0.068770
loss at step 2665: 0.060629
loss at step 2666: 0.098497
loss at step 2667: 0.064814
loss at step 2668: 0.053022
loss at step 2669: 0.064431
loss at step 2670: 0.065565
loss at step 2671: 0.099016
loss at step 2672: 0.110152
loss at step 2673: 0.103867
loss at step 2674: 0.115048
loss at step 2675: 0.067555
loss at step 2676: 0.081815
loss at step 2677: 0.071427
loss at step 2678: 0.100536
loss at step 2679: 0.077555
loss at step 2680: 0.083460
loss at step 2681: 0.061791
loss at step 2682: 0.043960
loss at step 2683: 0.054487
loss at step 2684: 0.038111
loss at step 2685: 0.106692
loss at step 2686: 0.120492
loss at step 2687: 0.118479
loss at step 2688: 0.146242
loss at step 2689: 0.120346
loss at step 2690: 0.106917
loss at step 2691: 0.095498
loss at step 2692: 0.070659
loss at step 2693: 0.062629
loss at step 2694: 0.070094
loss at step 2695: 0.053501
loss at step 2696: 0.088347
loss at step 2697: 0.098886
loss at step 2698: 0.078137
loss at step 2699: 0.088858
loss at step 2700: 0.059504
Average loss at step 2700: 0.087325, Accuracy = 0.8999999761581421
loss at step 2701: 0.063304
loss at step 2702: 0.083576
loss at step 2703: 0.061726
loss at step 2704: 0.062307
loss at step 2705: 0.073433
loss at step 2706: 0.041636
loss at step 2707: 0.075273
loss at step 2708: 0.063820
loss at step 2709: 0.052863
loss at step 2710: 0.066365
loss at step 2711: 0.060862
loss at step 2712: 0.080337
loss at step 2713: 0.058443
loss at step 2714: 0.053737
loss at step 2715: 0.061270
loss at step 2716: 0.063760
loss at step 2717: 0.104911
loss at step 2718: 0.211797
loss at step 2719: 0.175337
loss at step 2720: 0.173160
loss at step 2721: 0.097601
loss at step 2722: 0.079823
loss at step 2723: 0.073138
loss at step 2724: 0.099764
loss at step 2725: 0.089255
loss at step 2726: 0.104337
loss at step 2727: 0.062553
loss at step 2728: 0.060108
loss at step 2729: 0.049286
loss at step 2730: 0.064176
loss at step 2731: 0.075670
loss at step 2732: 0.120045
loss at step 2733: 0.113011
loss at step 2734: 0.114930
loss at step 2735: 0.102498
loss at step 2736: 0.084599
loss at step 2737: 0.086645
loss at step 2738: 0.065075
loss at step 2739: 0.052092
loss at step 2740: 0.063657
loss at step 2741: 0.053711
loss at step 2742: 0.077549
loss at step 2743: 0.078408
loss at step 2744: 0.063040
loss at step 2745: 0.064593
loss at step 2746: 0.069630
loss at step 2747: 0.052949
loss at step 2748: 0.068416
loss at step 2749: 0.056663
loss at step 2750: 0.061482
loss at step 2751: 0.057393
loss at step 2752: 0.041807
loss at step 2753: 0.073494
loss at step 2754: 0.076682
loss at step 2755: 0.068325
loss at step 2756: 0.063122
loss at step 2757: 0.067151
loss at step 2758: 0.057638
loss at step 2759: 0.061516
loss at step 2760: 0.053269
loss at step 2761: 0.062421
loss at step 2762: 0.062957
loss at step 2763: 0.060494
loss at step 2764: 0.143907
loss at step 2765: 0.131178
loss at step 2766: 0.134315
loss at step 2767: 0.113168
loss at step 2768: 0.080634
loss at step 2769: 0.075958
loss at step 2770: 0.093547
loss at step 2771: 0.101292
loss at step 2772: 0.083364
loss at step 2773: 0.071775
loss at step 2774: 0.051231
loss at step 2775: 0.041812
loss at step 2776: 0.057914
loss at step 2777: 0.047288
loss at step 2778: 0.098282
loss at step 2779: 0.100391
loss at step 2780: 0.094994
loss at step 2781: 0.107158
loss at step 2782: 0.082575
loss at step 2783: 0.078094
loss at step 2784: 0.055246
loss at step 2785: 0.048554
loss at step 2786: 0.041779
loss at step 2787: 0.062458
loss at step 2788: 0.061109
loss at step 2789: 0.082541
loss at step 2790: 0.069262
loss at step 2791: 0.052578
loss at step 2792: 0.059343
loss at step 2793: 0.035184
loss at step 2794: 0.054681
loss at step 2795: 0.049193
loss at step 2796: 0.040563
loss at step 2797: 0.043896
loss at step 2798: 0.035027
loss at step 2799: 0.044481
loss at step 2800: 0.045833
Average loss at step 2800: 0.074695, Accuracy = 0.9900000095367432
Optimization Finished!
Total time: 5271.551789283752 seconds
[Finished in 5279.0s]